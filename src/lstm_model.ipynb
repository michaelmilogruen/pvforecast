{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for PV Power Forecasting\n",
    "\n",
    "This notebook implements an LSTM (Long Short-Term Memory) neural network for forecasting photovoltaic power output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the processed training data\n",
    "data_path = '../data/processed_training_data.parquet'\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Loaded data shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "# Select relevant features for the model\n",
    "features = ['GHI', 'DHI', 'DNI', 'Temperature', 'WindSpeed', 'CloudCover']\n",
    "target = 'PowerOutput'\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in dataset:\")\n",
    "print(df[features + [target]].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data normalization\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "X = scaler_x.fit_transform(df[features])\n",
    "# Reshape target to 2D array for scaling\n",
    "y = scaler_y.fit_transform(df[[target]])\n",
    "\n",
    "# Save the scalers for later use\n",
    "joblib.dump(scaler_x, '../models/scaler_x.pkl')\n",
    "joblib.dump(scaler_y, '../models/scaler_y.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_sequences(X, y, time_steps=24):\n",
    "    \"\"\"\n",
    "    Create sequences of data for time series forecasting.\n",
    "    \n",
    "    Args:\n",
    "        X: Features array\n",
    "        y: Target array\n",
    "        time_steps: Number of time steps to look back\n",
    "        \n",
    "    Returns:\n",
    "        X_seq: Sequences of features\n",
    "        y_seq: Corresponding target values\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(len(X) - time_steps):\n",
    "        X_seq.append(X[i:i + time_steps])\n",
    "        y_seq.append(y[i + time_steps])\n",
    "        \n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create sequences for LSTM\n",
    "time_steps = 24  # Look back 24 time steps (e.g., hours)\n",
    "X_seq, y_seq = create_sequences(X, y, time_steps)\n",
    "\n",
    "print(f\"Sequence shapes - X: {X_seq.shape}, y: {y_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data into training and testing sets\n",
    "train_size = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:train_size], X_seq[train_size:]\n",
    "y_train, y_test = y_seq[:train_size], y_seq[train_size:]\n",
    "\n",
    "print(f\"Training set shapes - X: {X_train.shape}, y: {y_train.shape}\")\n",
    "print(f\"Testing set shapes - X: {X_test.shape}, y: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def build_lstm_model(input_shape, units=50, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Build an LSTM model for time series forecasting.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input data (time_steps, features)\n",
    "        units: Number of LSTM units\n",
    "        dropout_rate: Dropout rate for regularization\n",
    "        \n",
    "    Returns:\n",
    "        model: Compiled LSTM model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer with return sequences for stacking\n",
    "    model.add(LSTM(units=units, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(units=units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Build the LSTM model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (time_steps, features)\n",
    "model = build_lstm_model(input_shape)\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define hyperparameters to tune\n",
    "hyperparameters = {\n",
    "    'lstm_units': [32, 50, 64, 128],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'learning_rate': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Function to build and evaluate model with specific hyperparameters\n",
    "def evaluate_model(lstm_units, dropout_rate, batch_size, learning_rate):\n",
    "    # Build model with specified hyperparameters\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=lstm_units, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units=lstm_units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile with specified learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train with early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=20,  # Reduced epochs for faster tuning\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation data\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    return val_loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simple grid search implementation\n",
    "# Note: For a more comprehensive approach, consider using libraries like Keras Tuner or scikit-learn's GridSearchCV\n",
    "\n",
    "results = []\n",
    "\n",
    "# Uncomment to run the grid search (warning: may take significant time)\n",
    "'''\n",
    "for units in hyperparameters['lstm_units']:\n",
    "    for dropout in hyperparameters['dropout_rate']:\n",
    "        for batch in hyperparameters['batch_size']:\n",
    "            for lr in hyperparameters['learning_rate']:\n",
    "                print(f\"Testing: units={units}, dropout={dropout}, batch_size={batch}, lr={lr}\")\n",
    "                val_loss, _ = evaluate_model(units, dropout, batch, lr)\n",
    "                results.append({\n",
    "                    'lstm_units': units,\n",
    "                    'dropout_rate': dropout,\n",
    "                    'batch_size': batch,\n",
    "                    'learning_rate': lr,\n",
    "                    'val_loss': val_loss\n",
    "                })\n",
    "                \n",
    "# Convert results to DataFrame and sort by validation loss\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('val_loss')\n",
    "results_df.head(10)  # Show top 10 configurations\n",
    "'''\n",
    "\n",
    "# For demonstration, we'll use the following hyperparameters\n",
    "best_params = {\n",
    "    'lstm_units': 64,\n",
    "    'dropout_rate': 0.2,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "print(\"Using hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define callbacks for early stopping and model checkpointing\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='../models/best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot MAE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/lstm_training_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions and actual values\n",
    "y_test_inv = scaler_y.inverse_transform(y_test)\n",
    "y_pred_inv = scaler_y.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot predictions vs actual values\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot a subset of the test data for better visualization\n",
    "sample_size = min(500, len(y_test_inv))\n",
    "indices = np.arange(sample_size)\n",
    "\n",
    "plt.plot(indices, y_test_inv[:sample_size], 'b-', label='Actual Power Output')\n",
    "plt.plot(indices, y_pred_inv[:sample_size], 'r-', label='Predicted Power Output')\n",
    "plt.title('LSTM Model: Actual vs Predicted PV Power Output')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Power Output (kW)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('../results/lstm_predictions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a scatter plot of actual vs predicted values\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_test_inv, y_pred_inv, alpha=0.5)\n",
    "plt.plot([y_test_inv.min(), y_test_inv.max()], [y_test_inv.min(), y_test_inv.max()], 'r--')\n",
    "plt.title('LSTM Model: Actual vs Predicted Power Output')\n",
    "plt.xlabel('Actual Power Output (kW)')\n",
    "plt.ylabel('Predicted Power Output (kW)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('../results/lstm_scatter.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the model\n",
    "model.save('../models/power_forecast_model.keras')\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the implementation of an LSTM model for PV power forecasting. The model uses historical weather data and power output to predict future power generation. The performance metrics and visualizations provide insights into the model's accuracy and limitations.\n",
    "\n",
    "Next steps could include:\n",
    "1. Hyperparameter tuning to improve model performance\n",
    "2. Feature engineering to incorporate additional relevant variables\n",
    "3. Testing different model architectures (e.g., bidirectional LSTM, GRU)\n",
    "4. Implementing ensemble methods for more robust predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}