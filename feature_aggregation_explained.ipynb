{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Enhanced LSTM Feature Set with Statistical Aggregations\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook explains the process of creating an enhanced feature set for LSTM-based PV power prediction. The key innovation in this approach is incorporating statistical aggregations from high-resolution (10-minute) data into an hourly feature set. \n",
    "\n",
    "### Why Statistical Aggregations Matter\n",
    "\n",
    "Solar power generation is highly sensitive to short-term fluctuations in weather conditions:\n",
    "\n",
    "- Passing clouds can cause rapid drops in irradiance\n",
    "- Wind gusts affect panel temperature and efficiency\n",
    "- Brief precipitation events can reduce power output\n",
    "\n",
    "**Problem with Hourly Data**: Simple hourly averages smooth out these important variations, losing critical information.\n",
    "\n",
    "**Solution**: Create statistical aggregations (mean, max, min, standard deviation) from 10-minute data to capture these sub-hourly dynamics while maintaining hourly timestamps for the LSTM model.\n",
    "\n",
    "### Process Overview\n",
    "\n",
    "1. Load 10-minute resolution data\n",
    "2. Compute statistical aggregations (mean, max, min, std) for each hour\n",
    "3. Merge these aggregated features with previously selected 1-hour features\n",
    "4. Save the result as a new parquet file with hourly timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "'seaborn-whitegrid' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\micha\\anaconda3\\envs\\pvsim\\lib\\site-packages\\matplotlib\\style\\core.py:137\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     style \u001b[38;5;241m=\u001b[39m \u001b[43m_rc_params_in_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\micha\\anaconda3\\envs\\pvsim\\lib\\site-packages\\matplotlib\\__init__.py:894\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[1;34m(fname, transform, fail_on_error)\u001b[0m\n\u001b[0;32m    893\u001b[0m rc_temp \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_or_url(fname) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\micha\\anaconda3\\envs\\pvsim\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\micha\\anaconda3\\envs\\pvsim\\lib\\site-packages\\matplotlib\\__init__.py:871\u001b[0m, in \u001b[0;36m_open_file_or_url\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m    870\u001b[0m fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(fname)\n\u001b[1;32m--> 871\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seaborn-whitegrid'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Set plot styling\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseaborn-whitegrid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure.figsize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create directories for results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\micha\\anaconda3\\envs\\pvsim\\lib\\site-packages\\matplotlib\\style\\core.py:139\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    137\u001b[0m         style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid package style, path of style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile, URL of style file, or library style name (library \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyles are listed in `style.available`)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    143\u001b[0m filtered \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m style:  \u001b[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: 'seaborn-whitegrid' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set plot styling\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Create directories for results\n",
    "if not os.path.exists('results/feature_analysis'):\n",
    "    os.makedirs('results/feature_analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "We start by loading two datasets:\n",
    "1. The high-resolution 10-minute interval data (`station_data_10min.parquet`)\n",
    "2. The previously created hourly feature set (`lstm_features.parquet`)\n",
    "\n",
    "The 10-minute data provides more granular information about weather conditions and power output, while the 1-hour data contains the previously selected features for LSTM modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "df_10min = pd.read_parquet('data/station_data_10min.parquet')\n",
    "df_1h = pd.read_parquet('data/lstm_features.parquet')\n",
    "\n",
    "print(f\"10-minute data: {df_10min.shape} rows, {df_10min.shape[1]} columns\")\n",
    "print(f\"1-hour data: {df_1h.shape} rows, {df_1h.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the 10-minute data to understand what's available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the columns and first few rows of the 10-minute data\n",
    "print(\"\\nColumns in 10-minute data:\")\n",
    "print(df_10min.columns.tolist())\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nSample of 10-minute data:\")\n",
    "df_10min.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the hourly feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the columns and first few rows of the hourly data\n",
    "print(\"\\nColumns in hourly data:\")\n",
    "print(df_1h.columns.tolist())\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nSample of hourly data:\")\n",
    "df_1h.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing Sub-hourly Variations\n",
    "\n",
    "Before we compute statistical aggregations, let's visualize why they're important. We'll plot a single day of 10-minute data to see the variations that would be lost in hourly averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single day for visualization\n",
    "# Choose a day with interesting variations (e.g., partially cloudy)\n",
    "sample_date = '2023-06-15'  # You can adjust this to any date in your dataset\n",
    "day_data = df_10min[sample_date]\n",
    "\n",
    "# If the date doesn't exist, choose the first summer day in the dataset\n",
    "if len(day_data) == 0:\n",
    "    summer_days = df_10min[df_10min.index.month.isin([6, 7, 8])]\n",
    "    if len(summer_days) > 0:\n",
    "        first_summer_day = summer_days.index[0].date()\n",
    "        day_data = df_10min[df_10min.index.date == first_summer_day]\n",
    "    else:\n",
    "        # Just take the first day with daylight\n",
    "        day_data = df_10min[df_10min['GlobalRadiation [W m-2]'] > 100].iloc[:144]  # 24 hours of 10-min data\n",
    "\n",
    "# Plot key variables for the day\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Plot 10-minute data\n",
    "day_data['GlobalRadiation [W m-2]'].plot(ax=axes[0, 0], title='Global Radiation (10-min intervals)')\n",
    "axes[0, 0].set_ylabel('W/m²')\n",
    "\n",
    "day_data['power_w'].plot(ax=axes[0, 1], title='Power Output (10-min intervals)', color='green')\n",
    "axes[0, 1].set_ylabel('Watts')\n",
    "\n",
    "# Calculate hourly averages for comparison\n",
    "hourly_data = day_data.resample('1H').mean()\n",
    "\n",
    "# Plot the hourly data alongside 10-min data for comparison\n",
    "axes[1, 0].plot(day_data.index, day_data['GlobalRadiation [W m-2]'], 'b-', alpha=0.5, label='10-min data')\n",
    "axes[1, 0].plot(hourly_data.index, hourly_data['GlobalRadiation [W m-2]'], 'ro-', label='Hourly average')\n",
    "axes[1, 0].set_title('Global Radiation: 10-min vs Hourly')\n",
    "axes[1, 0].set_ylabel('W/m²')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].plot(day_data.index, day_data['power_w'], 'g-', alpha=0.5, label='10-min data')\n",
    "axes[1, 1].plot(hourly_data.index, hourly_data['power_w'], 'ro-', label='Hourly average')\n",
    "axes[1, 1].set_title('Power Output: 10-min vs Hourly')\n",
    "axes[1, 1].set_ylabel('Watts')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the information loss\n",
    "radiation_hourly_std = day_data.resample('1H')['GlobalRadiation [W m-2]'].std().mean()\n",
    "power_hourly_std = day_data.resample('1H')['power_w'].std().mean()\n",
    "\n",
    "print(f\"Average standard deviation within each hour:\")\n",
    "print(f\"  - Global Radiation: {radiation_hourly_std:.2f} W/m²\")\n",
    "print(f\"  - Power Output: {power_hourly_std:.2f} W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above show how much information is lost when using only hourly averages. The standard deviation within each hour quantifies this variation. This is why we need statistical aggregations to capture:\n",
    "\n",
    "1. **Maximum values** - Peak irradiance and power production potential\n",
    "2. **Minimum values** - Worst-case scenarios within the hour\n",
    "3. **Standard deviation** - Volatility and stability measures\n",
    "\n",
    "## 3. Defining Features to Aggregate\n",
    "\n",
    "Now we'll define which features to aggregate and which statistical functions to apply to each feature. We select features that:\n",
    "\n",
    "1. Are physically relevant to PV power generation\n",
    "2. Show significant sub-hourly variations\n",
    "3. Represent different aspects of weather conditions\n",
    "\n",
    "For each feature, we apply specific aggregation functions based on their physical meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to aggregate\n",
    "# These are key weather and irradiance features that can benefit from \n",
    "# capturing sub-hourly variations\n",
    "features_to_aggregate = [\n",
    "    'GlobalRadiation [W m-2]',     # Primary driver of PV generation\n",
    "    'Temperature [degree_Celsius]', # Affects panel efficiency\n",
    "    'WindSpeed [m s-1]',           # Affects panel cooling\n",
    "    'ClearSkyIndex',               # Indicates cloud effects\n",
    "    'Precipitation [mm]'           # Indicates rain events\n",
    "]\n",
    "\n",
    "# Define aggregation functions for each feature\n",
    "aggregation_dict = {\n",
    "    'GlobalRadiation [W m-2]': ['mean', 'max', 'min', 'std'],  # All statistics relevant for radiation\n",
    "    'Temperature [degree_Celsius]': ['mean', 'max', 'min', 'std'],  # Temperature extremes affect efficiency\n",
    "    'WindSpeed [m s-1]': ['mean', 'max', 'std'],  # Wind peaks important for cooling\n",
    "    'ClearSkyIndex': ['mean', 'min', 'std'],  # Min captures worst cloud cover\n",
    "    'Precipitation [mm]': ['sum', 'max']  # Sum captures total rainfall, max captures intensity\n",
    "}\n",
    "\n",
    "# Display the aggregation strategy\n",
    "print(\"Aggregation strategy for each feature:\")\n",
    "for feature, aggs in aggregation_dict.items():\n",
    "    print(f\"  - {feature}: {', '.join(aggs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Aggregation Rationale\n",
    "\n",
    "- **GlobalRadiation**: All statistics are valuable as radiation directly drives PV generation. Max captures peak potential, min captures cloud coverage, std captures variability.\n",
    "  \n",
    "- **Temperature**: Panel efficiency decreases with higher temperatures. Max captures worst-case efficiency scenarios, min captures best-case efficiency scenarios.\n",
    "  \n",
    "- **WindSpeed**: Affects panel cooling. Max captures gusts that could significantly cool panels or potentially cause mechanical stress.\n",
    "  \n",
    "- **ClearSkyIndex**: Ratio of actual radiation to theoretical clear-sky radiation. Min captures worst cloud cover within the hour.\n",
    "  \n",
    "- **Precipitation**: Sum captures total rainfall in the hour, max captures rainfall intensity, both relevant for panel soiling and cleaning.\n",
    "\n",
    "## 4. Computing Statistical Aggregations\n",
    "\n",
    "Now we'll compute the aggregations for each feature. We process them individually to manage memory usage efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Computing statistical aggregations for {len(features_to_aggregate)} features...\")\n",
    "\n",
    "# First ensure the index is DatetimeIndex for proper resampling\n",
    "df_10min = df_10min.sort_index()\n",
    "\n",
    "# Create a new list to store the aggregated dataframes\n",
    "agg_dfs = []\n",
    "\n",
    "# For each feature, compute the specified aggregations\n",
    "for feature, aggs in aggregation_dict.items():\n",
    "    # Select just this feature to reduce memory usage during aggregation\n",
    "    feature_df = df_10min[[feature]]\n",
    "    \n",
    "    # Create a dictionary mapping the feature to its aggregation functions\n",
    "    agg_dict = {feature: aggs}\n",
    "    \n",
    "    # Resample to hourly frequency with the specified aggregations\n",
    "    # Note: '1h' is the proper notation (lowercase h)\n",
    "    hourly_agg = feature_df.resample('1h').agg(agg_dict)\n",
    "    \n",
    "    # Flatten the column names\n",
    "    hourly_agg.columns = [f\"{feature}_{agg}\" for agg in aggs]\n",
    "    \n",
    "    # Add to the list of aggregated dataframes\n",
    "    agg_dfs.append(hourly_agg)\n",
    "    \n",
    "    # Show progress\n",
    "    print(f\"  - Aggregated {feature} with {len(aggs)} functions: {', '.join(aggs)}\")\n",
    "\n",
    "# Combine all aggregated features into a single dataframe\n",
    "df_agg = pd.concat(agg_dfs, axis=1)\n",
    "\n",
    "print(f\"\\nCreated aggregated features dataframe with shape: {df_agg.shape}\")\n",
    "print(f\"Columns: {df_agg.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "We need to check for and handle any missing values in the aggregated data to ensure data quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values and handle them\n",
    "missing_counts = df_agg.isna().sum()\n",
    "total_missing = missing_counts.sum()\n",
    "\n",
    "print(f\"Missing values per column:\")\n",
    "for col, count in missing_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"  - {col}: {count}\")\n",
    "        \n",
    "print(f\"\\nTotal missing values: {total_missing}\")\n",
    "\n",
    "if total_missing > 0:\n",
    "    print(f\"Handling {total_missing} missing values in aggregated data...\")\n",
    "    # Forward fill then backward fill to handle missing values\n",
    "    df_agg = df_agg.ffill().bfill()\n",
    "    print(f\"Remaining missing values after filling: {df_agg.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merging with Hourly Features\n",
    "\n",
    "Now we merge the aggregated features with the existing hourly features. First, we need to ensure the timestamps align:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine timestamps to ensure alignment\n",
    "print(f\"Hourly data index range: {df_1h.index.min()} to {df_1h.index.max()}\")\n",
    "print(f\"Aggregated data index range: {df_agg.index.min()} to {df_agg.index.max()}\")\n",
    "\n",
    "# Ensure the indexes (timestamps) match by using the index from 1h data\n",
    "df_agg = df_agg.reindex(df_1h.index)\n",
    "\n",
    "# Check for any missing values after reindexing\n",
    "missing_after_reindex = df_agg.isna().sum().sum()\n",
    "if missing_after_reindex > 0:\n",
    "    print(f\"Handling {missing_after_reindex} missing values after reindexing...\")\n",
    "    df_agg = df_agg.ffill().bfill()\n",
    "\n",
    "# Combine with the existing 1-hour data\n",
    "df_combined = pd.concat([df_1h, df_agg], axis=1)\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_combined.shape}\")\n",
    "print(f\"Number of features: {df_combined.shape[1] - 1}\")  # Excluding power_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the complete set of features in our enhanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all features in the combined dataset\n",
    "print(\"Features in the combined dataset:\")\n",
    "for column in df_combined.columns:\n",
    "    print(f\"- {column}\")\n",
    "\n",
    "# Count features by category\n",
    "base_features = [col for col in df_combined.columns if not ('_mean' in col or '_max' in col\n",
    "                                                    or '_min' in col or '_std' in col\n",
    "                                                    or '_sum' in col)]\n",
    "agg_features = [col for col in df_combined.columns if ('_mean' in col or '_max' in col\n",
    "                                                or '_min' in col or '_std' in col\n",
    "                                                or '_sum' in col)]\n",
    "\n",
    "print(f\"\\nFeature counts by category:\")\n",
    "print(f\"  - Base features: {len(base_features)}\")\n",
    "print(f\"  - Aggregated features: {len(agg_features)}\")\n",
    "print(f\"  - Total: {len(base_features) + len(agg_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyzing the Enhanced Feature Set\n",
    "\n",
    "Now let's analyze our enhanced feature set to understand the relationships between features and power output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with power_w\n",
    "correlation = df_combined.corr()['power_w'].sort_values(ascending=False)\n",
    "correlation = correlation.drop('power_w')  # Remove self-correlation\n",
    "\n",
    "print(\"Top 10 features by correlation with power_w:\")\n",
    "print(correlation.head(10))\n",
    "\n",
    "print(\"\\nBottom 5 features by correlation with power_w:\")\n",
    "print(correlation.tail(5))\n",
    "\n",
    "# Plot top 20 correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_n = 20\n",
    "correlation_top = correlation.head(top_n)\n",
    "correlation_top.plot(kind='barh')\n",
    "plt.title(f'Top {top_n} Features by Correlation with power_w')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/feature_analysis/top_correlations_with_aggregations.png')\n",
    "plt.show()\n",
    "\n",
    "# Calculate how many aggregated features are in the top 10\n",
    "agg_in_top10 = sum(1 for feature in correlation.head(10).index if any(suffix in feature for suffix in ['_mean', '_max', '_min', '_std', '_sum']))\n",
    "print(f\"\\nAggregated features in top 10: {agg_in_top10} out of 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis\n",
    "\n",
    "Let's examine if the statistical aggregations provide additional predictive power beyond the base features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base features vs aggregated features\n",
    "# Group features by their base name\n",
    "feature_groups = {}\n",
    "for col in df_combined.columns:\n",
    "    if col == 'power_w':\n",
    "        continue\n",
    "        \n",
    "    # Extract base feature name\n",
    "    if any(suffix in col for suffix in ['_mean', '_max', '_min', '_std', '_sum']):\n",
    "        base_name = col.split('_')[0] + ' ' + ' '.join(col.split(' ')[1:-1])\n",
    "        if base_name not in feature_groups:\n",
    "            feature_groups[base_name] = []\n",
    "        feature_groups[base_name].append(col)\n",
    "    else:\n",
    "        if col not in feature_groups:\n",
    "            feature_groups[col] = []\n",
    "        feature_groups[col].append(col)\n",
    "\n",
    "# For each feature group, find the highest correlated feature\n",
    "best_features = {}\n",
    "for base, features in feature_groups.items():\n",
    "    if len(features) > 0:\n",
    "        correlations = correlation[features]\n",
    "        best_feature = correlations.idxmax()\n",
    "        best_corr = correlations.loc[best_feature]\n",
    "        best_features[base] = (best_feature, best_corr)\n",
    "\n",
    "# Sort by correlation magnitude\n",
    "sorted_best = sorted(best_features.items(), key=lambda x: abs(x[1][1]), reverse=True)\n",
    "\n",
    "print(\"Best feature for each base feature group:\")\n",
    "for base, (feature, corr) in sorted_best:\n",
    "    is_agg = 'Yes' if any(suffix in feature for suffix in ['_mean', '_max', '_min', '_std', '_sum']) else 'No'\n",
    "    print(f\"{base:30} -> {feature:40} (corr: {corr:.3f}, aggregated: {is_agg})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Value of Aggregated Features\n",
    "\n",
    "Let's visualize how the aggregated features capture information that the base features miss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a high-correlation aggregated feature to visualize\n",
    "agg_features_corr = [(f, c) for f, c in correlation.items() \n",
    "                     if any(suffix in f for suffix in ['_mean', '_max', '_min', '_std', '_sum'])]\n",
    "top_agg_feature, top_agg_corr = sorted(agg_features_corr, key=lambda x: abs(x[1]), reverse=True)[0]\n",
    "\n",
    "# Get the base feature name\n",
    "base_feature = top_agg_feature.split('_')[0]\n",
    "for part in top_agg_feature.split(' ')[1:]:\n",
    "    if not any(suffix == part for suffix in ['mean', 'max', 'min', 'std', 'sum']):\n",
    "        base_feature += ' ' + part\n",
    "\n",
    "# Check if base feature exists in dataset\n",
    "if base_feature in df_combined.columns:\n",
    "    # Select a day with significant variation\n",
    "    sample_day = df_combined[df_combined[top_agg_feature].std() > df_combined[top_agg_feature].std().mean()].index[0].date()\n",
    "    day_data = df_combined[df_combined.index.date == sample_day]\n",
    "    \n",
    "    # Plot base feature vs aggregated feature vs power\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Feature Value', color=color)\n",
    "    ax1.plot(day_data.index, day_data[base_feature], color=color, label=base_feature)\n",
    "    ax1.plot(day_data.index, day_data[top_agg_feature], color='tab:orange', label=top_agg_feature)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "    \n",
    "    # Create a second y-axis for power\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:green'\n",
    "    ax2.set_ylabel('Power (W)', color=color)\n",
    "    ax2.plot(day_data.index, day_data['power_w'], color=color, label='power_w')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    plt.title(f'Base vs Aggregated Feature: {base_feature} vs {top_agg_feature}\\nDate: {sample_day}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/feature_analysis/base_vs_aggregated_feature.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate correlation improvement\n",
    "    base_corr = correlation.get(base_feature, 0)\n",
    "    agg_corr = correlation.get(top_agg_feature, 0)\n",
    "    print(f\"Correlation improvement: {base_feature} ({base_corr:.3f}) vs {top_agg_feature} ({agg_corr:.3f})\")\n",
    "    print(f\"Absolute improvement: {abs(agg_corr) - abs(base_corr):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving the Enhanced Feature Set\n",
    "\n",
    "Finally, let's save our enhanced feature set to a parquet file for use in LSTM modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final dataset\n",
    "output_path = 'data/lstm_features_with_aggregations.parquet'\n",
    "df_combined.to_parquet(output_path)\n",
    "print(f\"Dataset with hourly features and sub-hourly statistical aggregations saved to {output_path}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics of key features:\")\n",
    "selected_cols = ['power_w', 'GlobalRadiation [W m-2]', 'GlobalRadiation [W m-2]_max', \n",
    "                 'Temperature [degree_Celsius]', 'Temperature [degree_Celsius]_max',\n",
    "                 'ClearSkyIndex', 'ClearSkyIndex_min']\n",
    "summary_stats = df_combined[selected_cols].describe()\n",
    "summary_stats.T[['mean', 'std', 'min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "We've successfully created an enhanced feature set for LSTM-based PV power prediction by incorporating statistical aggregations from 10-minute data into an hourly feature set. This approach captures important sub-hourly variations that would be lost in simple hourly averages.\n",
    "\n",
    "### Key Benefits of the Enhanced Feature Set\n",
    "\n",
    "1. **Captures Sub-hourly Dynamics**: The statistical aggregations preserve information about short-term fluctuations in weather conditions that affect PV power generation.\n",
    "\n",
    "2. **Maintains Hourly Timestamps**: By aggregating to hourly timestamps, we keep the dataset at a manageable size for LSTM modeling while preserving key information.\n",
    "\n",
    "3. **Improved Feature Correlations**: Several aggregated features show higher correlation with power output than the base hourly features.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Use this enhanced feature set in LSTM model training\n",
    "2. Compare model performance with and without the aggregated features\n",
    "3. Consider feature selection techniques to identify the most important subset of features\n",
    "4. Explore different sequence lengths for the LSTM model to capture temporal dependencies\n",
    "\n",
    "The final dataset `lstm_features_with_aggregations.parquet` is now ready for use in LSTM modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
